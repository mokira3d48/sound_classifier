{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd692b2-3e3f-4998-b8a4-8a9bdc581dd6",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1191ff7-00de-47e7-ad93-8729dd0f73e5",
   "metadata": {},
   "source": [
    "La classification des sons est l'une des applications les plus utilisées dans **Audio Deep Learning**. Cela implique d'apprendre à classer les sons et à prédire la catégorie de ce son. Ce type de problème peut être appliqué à de nombreux scénarios pratiques, par exemple la classification de clips musicaux pour identifier le genre de la musique, ou la classification de courts énoncés par un ensemble de locuteurs pour identifier le locuteur en fonction de la voix.\n",
    "\n",
    "Tout comme la classification des chiffres écrits à la main à l'aide de l'ensemble de données MNIST considérée comme un problème de type \"Hello World\" pour introduire la vision par ordinateur, la classification audio est lui aussi considérée comme le problème d'introduction pour Audio Deep Learning.\n",
    "\n",
    "Dans cet article, nous allons parcourir une application de démonstration simple afin de comprendre l'approche utilisée pour résoudre ces problèmes de classification audio. Mon objectif tout au long de ce tutoriel sera de te faire comprendre non seulement comment ces programmes fonctionnent, mais aussi, pourquoi ils fonctionnent de cette façon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05110556-86fe-4d9b-b1c3-501d0d1d414c",
   "metadata": {},
   "source": [
    "# Les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c914e0a7-4d21-49c0-84f3-e70c1670a43f",
   "metadata": {},
   "source": [
    "Il existe de nombreux ensembles de données appropriés disponibles pour les sons de différents types. Ces ensembles de données contiennent un grand nombre d'échantillons audio, ainsi qu'une étiquette de classe pour chaque échantillon qui identifie le type de son dont il s'agit, en fonction du problème qu'on essaie de résoudre. Ces étiquettes de classe peuvent souvent être obtenues à partir d'une partie du nom de fichier de l'échantillon audio ou du nom du sous-dossier dans lequel se trouve le fichier. Alternativement, les étiquettes de classe sont spécifiées dans un fichier de métadonnées séparé, généralement au format TXT, JSON ou CSV.\n",
    "\n",
    "Pour notre exemple de projet, nous allons utiliser l'ensemble de données [**Urban Sound 8K**](https://urbansounddataset.weebly.com/urbansound8k.html) qui consiste en un corpus de sons ordinaires enregistrés qu'on retrouve dans notre vie quotidienne en ville. Les sons sont rangés en 10 classes telles que le forage, les aboiements de chiens et les sirènes. Chaque échantillon sonore est étiqueté avec la classe à laquelle il appartient.\n",
    "\n",
    "Après avoir téléchargé ce jeu de données, on remarque qu'il se compose de deux parties :\n",
    "\n",
    "- Les **fichiers audio** dans le dossier `audio` qui comporte 10 sous-dossiers nommés de `fold1` à `fold10`. Dans chaque sous-dossier on trouve un certain nombre d'échantillons audio avec l'extension **.wav**, par exemple : `pli1/103074–7–1–0.wav`. Chaque échantillons durent environ 4 secondes.\n",
    "- Les **métadonnées** dans le dossier `metadata` qui contient un fichier nommé `UrbanSound8K.csv` qui contient des informations sur chaque échantillon audio du jeu de données, telles que son nom de fichier, son étiquette de classe, l'emplacement du sous-dossier 'fold', etc. L'étiquette de classe est un entier qui vari de 0 à 9 pour chacune des 10 classes, par exemple : le chiffre 0 désigne climatiseur, 1 : un klaxon de voiture, etc.\n",
    "\n",
    "Comme pour la plupart des problèmes de deep learning, nous suivrons les étapes présentées dans la figure suivante :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde321a-0e3f-4fbd-9d6a-73aa68c5ee34",
   "metadata": {},
   "source": [
    "![Flux de travail d'apprentissage en profondeur](images/dl_steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09081298-38ea-4dfe-900f-8146e4d1d04e",
   "metadata": {},
   "source": [
    "Pour notre tâche de classification, les caractéritiques $X$ désigneront les chemins vers des fichiers audio et les étiquettes cibles $y$ vont désignés les noms de classe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fb97bd-7ec8-4013-a92a-34bcbcd820ac",
   "metadata": {},
   "source": [
    "## Récupération des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720d0ad-46c2-4726-a199-bdc08689861f",
   "metadata": {},
   "source": [
    "Pour télécharger l'ensemble de données, on va juste exécuter la commande suivante : \n",
    "```sh\n",
    "wget -c https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz\n",
    "```\n",
    "Voici le code à exécuter dans un noteboock :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb9d228-d2e5-4622-b582-d27825e16471",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e9e4e2-0c0c-4ef2-a53c-ae681da6f374",
   "metadata": {},
   "source": [
    "## Préparation des données d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196760da-a273-4f45-aaa1-9861f093b490",
   "metadata": {},
   "source": [
    "De nombreux ensembles de données se composent uniquement de fichiers audio organisés dans une structure de dossiers à partir de laquelle on peut extraire les étiquettes de classe. Pour préparer nos données d'entraînement dans le format necessaire pour notre modèle, nous procédera en trois (03) étapes :\n",
    "\n",
    "- Analyser le répertoire pour préparez une liste de tous les chemins de fichiers audio;\n",
    "- Extraire l'étiquette de classe de chaque fichier audio du nom de son dossier parent;\n",
    "- Faire correspondre chaque nom de classe à un numéro unique.\n",
    "\n",
    "Étant donné que l'ensemble de données contient déjà un fichier CSV de métadonnées contenant les informations recherchées sur chaque fichier audio, on va l'utiliser directement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f84e1d68-8500-4646-bcc6-50c7ce45c5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# On ouvre et on affiche les 5 premiers lignes du fichier.\n",
    "metadata_df = pd.read_csv(\"UrbanSound8K.csv\")\n",
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e44828d-ef19-45c5-a3d6-abbbc867a56c",
   "metadata": {},
   "source": [
    "On a besoin d'une créer une colonnes de plus pour préparer les données à d'entraînement. Il s'agit de la colonne qui contiendra le chemin d'accès vers chaque fichier audio. Pour cela, on va ajouter une nouvelle colonne nommée `'relative_path'` qui représentera le chemin relatif vers chaque fichier audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33629139-afd5-475f-9e5c-f35ad565a65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/fold5/100032-3-0-0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/fold5/100263-2-0-117.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/fold5/100263-2-0-121.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/fold5/100263-2-0-126.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/fold5/100263-2-0-137.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               relative_path\n",
       "0    /fold5/100032-3-0-0.wav\n",
       "1  /fold5/100263-2-0-117.wav\n",
       "2  /fold5/100263-2-0-121.wav\n",
       "3  /fold5/100263-2-0-126.wav\n",
       "4  /fold5/100263-2-0-137.wav"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On construit le chemin relatif vers chaque fichier audio.\n",
    "metadata_df['relative_path'] = '/fold' + metadata_df['fold'].astype(str) \\\n",
    "        + '/' \\\n",
    "        + metadata_df['slice_file_name'].astype(str) \\\n",
    "\n",
    "# On affiche un apperçu.\n",
    "metadata_df[['relative_path']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b675ff3-7d1b-4bb9-b1f1-fcaafaa17ddd",
   "metadata": {},
   "source": [
    "On a juste besoin de deux colonnes dans cette table de données pour constituer la dataset d'entraînement, donc récupérons les dans une nouvelle variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "406c3204-466a-43d0-b6fa-bf6aa7e542e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "      <th>classID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/fold5/100032-3-0-0.wav</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/fold5/100263-2-0-117.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/fold5/100263-2-0-121.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/fold5/100263-2-0-126.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/fold5/100263-2-0-137.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               relative_path  classID\n",
       "0    /fold5/100032-3-0-0.wav        3\n",
       "1  /fold5/100263-2-0-117.wav        2\n",
       "2  /fold5/100263-2-0-121.wav        2\n",
       "3  /fold5/100263-2-0-126.wav        2\n",
       "4  /fold5/100263-2-0-137.wav        2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On sélectionne donc les colonnes 'relative_path' et `'classID'`\n",
    "dataset_df = metadata_df[['relative_path', 'classID']]\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98831716-ab5d-490d-b85f-f3b7c8a71044",
   "metadata": {},
   "source": [
    "Le faite d'avoir le chemin d'accès vers les fichiers audio va nous faciliter les choses pour préparation des données d'apprentissage.\n",
    "\n",
    "En effet, on ne peut pas faire entrer des chemins de fichiers audio directement dans le modèle. Nous devons donc lire et charger les données audio de chaque fichier et les traiter afin qu'elles soient dans un format exploitable par le modèle.\n",
    "\n",
    "Ce prétraitement audio sera entièrement effectué dynamiquement au moment de l'exécution de programme d'entraînement. Cette approche est similaire à celle des fichiers images en vision par ordinateur. Car, comme les données d'image, les données audio sont assez volumineuses et gourmandes en mémoire, ce qui fait qu'on ne peut pas charger l'intégralité de l'ensemble de données en mémoire à l'avance. Donc, les seules informations qu'on arriver à conserver en mémoire sont les chemins d'accès vers ces fichiers audio (ou image).\n",
    "\n",
    "Les données audio sont traitées en appliquant une série de transformations. Exactement comme pour les images, nous pourrions un pipeline de transformations constitué d'une liste de fonctions de traitement audio.\n",
    "\n",
    "Voici la liste des fonctions à programmer :\n",
    "\n",
    "1. Lecture de fichier audio;\n",
    "2. Conversion en 2 canaux (stereo);\n",
    "3. Normalisation du taux d'échantillonnage;\n",
    "4. Redimensionnement à la même longueur (durée);\n",
    "5. Décalage temporel pour l'augmentation;\n",
    "6. Génération de Spectrogramme Mel;\n",
    "7. Masquage temporel et fréquentiel pour l'augmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aa8f6b-7718-4fc8-9575-cfe8a702a5f5",
   "metadata": {},
   "source": [
    "### Lecture de fichier audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c67ad3-e782-4bca-9654-351a3207d728",
   "metadata": {},
   "source": [
    "La première chose dont nous avons besoin est de lire et de charger le fichier audio au format **.wav**. Puisque nous utilisons [**Pytorch**](https://pytorch.org/) pour cet exemple, l'implémentation ci-dessous utilise `torchaudio` pour le traitement audio, mais le package python `librosa` peut aussi faire l'affaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f51bdfe-9b26-4476-b9d5-a819a0de2316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "\n",
    "\n",
    "def read(audio_file: str) -> Tuple[torch.Tensor, int]:\n",
    "    \"\"\" Fonction de lecture des données audio.\n",
    "\n",
    "    On utilise la fonction `load()` de `torchaudio`.\n",
    "    Pour plus d'information, voici un lien :\n",
    "    https://pytorch.org/audio/stable/tutorials/audio_io_tutorial.html#loading-audio-data\n",
    "\n",
    "    Arguments:\n",
    "        audio_file (`str`): Le chemin d'accès vers le fichier audio.\n",
    "\n",
    "    Returns:\n",
    "        `tuple`: constitué d'un tenseur et d'un entier.\n",
    "           * Le tenseur nommé `waveform` représente les données audio;\n",
    "           * et l'entier nommé `sample_rate` représente le taux\n",
    "             d'échantillonnage.\n",
    "\n",
    "    Example:\n",
    "        >>> waveform, sample_rate = read('./pathto/audio_file.wav')\n",
    "        >>> print(sample_rate)\n",
    "        44100\n",
    "    \"\"\"\n",
    "    waveform, sample_rate = torchaudio.load(audio_file)\n",
    "    return (waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c961c5cc-95d7-46c9-9ee0-b8cb0200c75d",
   "metadata": {},
   "source": [
    "### Conversion en 2 canaux (stereo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1895484-2668-4621-b961-0007f18afb11",
   "metadata": {},
   "source": [
    "Dans un ensemble de données de fichiers audio, il peut arriver qu'on ait certains fichiers son sont **mono** (c'est-à-dire 1 canal audio) tandis que la plupart d'entre eux soient **stéréo** (c'est-à-dire 2 canaux audio). Étant donné que notre modèle s'attend à ce que tous les données d'entrées aient les mêmes dimensions, alors nous sommes amenés à convertir les fichiers audio mono en stéréo, en dupliquant juste le premier canal sur le second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3758d581-a95e-4542-8b60-6a9dd44acf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "\n",
    "\n",
    "def rechannel(audio: Tuple[torch.Tensor, int],\n",
    "              channel_count: int) -> Tuple[torch.Tensor, int]:\n",
    "    \"\"\" Fonction de conversion de canal.\n",
    "\n",
    "    La conversion de mono en stéréo revient juste\n",
    "    à concatener une copie du tableau de données\n",
    "    audio (waveform) à lui-même.\n",
    "\n",
    "    Arguments:\n",
    "        audio (:obj:`tuple`): Le tuple waveform et sample_rate.\n",
    "        channel_count (`int`): Le nombre de cannaux souhaités.\n",
    "\n",
    "    Returns:\n",
    "        `tuple`: Le couple formé du tenseur modifié\n",
    "                 et du taux d'échantillonnage.\n",
    "\n",
    "    Example:\n",
    "        >>> audio = read('./pathto/audio_file.wav')\n",
    "        >>> audio_mono = rechannel(audio, 1)  # converti en mono.\n",
    "        >>> audio_stereo = rechannel(audio, 2)  # converti en stéréo.\n",
    "        >>>\n",
    "    \"\"\"\n",
    "    if channel_count not in (1, 2):\n",
    "        raise ValueError(\"Le nombre de channal doit être 1 ou 2.\")\n",
    "\n",
    "    wave_form, sample_rate = audio\n",
    "    # NOTE: Le nombre de cannal est égal\n",
    "    #       au nombre de lignes du tableau wave_form.\n",
    "    if wave_form.shape[0] == channel_count:\n",
    "        # Plus la peine de continuer si le nombre de cannaux de ce audio\n",
    "        # est déjà le nombre de cannal souhaité.\n",
    "        return audio\n",
    "\n",
    "    wave_form_rech = None\n",
    "    if channel_count == 2:\n",
    "        # on convertie en stéréo:\n",
    "        wave_form_rech = torch.cat([wave_form, wave_form])\n",
    "    elif channel_count == 1:\n",
    "        # on convertie en mono:\n",
    "        wave_form_rech = wave_form[:1, :]\n",
    "\n",
    "    return (wave_form_rech, sample_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957526a1-0b80-4c2d-8083-dbf5b44034eb",
   "metadata": {},
   "source": [
    "### Normalisation du taux d'échantillonnage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e151b64-e158-4e90-b9ee-f72e7657da89",
   "metadata": {},
   "source": [
    "Toujours dans cette même ensemble de données, certains fichiers audio sont échantionnés à une fréquence de 48000 Hz, alors que la plupart sont échantillonnés à une fréquence de 44100 Hz.\n",
    "\n",
    "Pour rappel, en mécanique, la fréquence est le nombre de tours enrégistrés en une seconde et un tour correspond une période du signal. En traitement numérique du signal, les signaux audio sont généralement échantillonnés à une fréquence de **44100 Hz**, ce qui signifie les données audio récupérées en une seconde sont stockées dans un **tableau de taille 44100**.\n",
    "\n",
    "Donc, pour certains fichiers audio de notre dataset, une seconde d'audio aura une taille de tableau de 48 000, or qu'elle devrait avoir une taille de tableau plus petite, 44 100 comme la majorité. Ce sont ces raisons qui nous amène à vouloir normaliser en convertissant tout l'audio au même taux d'échantillonnage afin que tous les tableaux aient les mêmes dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6106b92-f17c-40a3-9776-fc8ce8717e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "from torchaudio import transforms as T\n",
    "\n",
    "\n",
    "def resample(audio: Tuple[torch.Tensor, int],\n",
    "             new_sample_rate: int) -> Tuple[torch.Tensor, int]:\n",
    "    \"\"\" Fonction de normalisation de la fréquence d'échantionnage.\n",
    "\n",
    "    On va utiliser la classe `Resample` du module `transforms`\n",
    "    du package `torchaudio` pour effectuer cette normalisation.\n",
    "\n",
    "    Arguments:\n",
    "        audio (:obj:`tuple`): Le tuple waveform et sample_rate.\n",
    "        new_sample_rate (`int`): La nouvelle fréquence d'échantillonnage\n",
    "                                 vers laquelle on veut convertir les données\n",
    "                                 audio.\n",
    "\n",
    "    Returns:\n",
    "        `tuple`: Le couple formé du tenseur modifié\n",
    "                 et du taux d'échantillonnage.\n",
    "\n",
    "    Example:\n",
    "        >>> audio = read('./pathto/audio_file.wav')\n",
    "        >>> audio = resample(audio, 44100)\n",
    "        >>>\n",
    "    \"\"\"\n",
    "    wave_form, sample_rate = audio\n",
    "    if sample_rate == new_sample_rate:\n",
    "        # Il n'y plus rien à faire d'autre.\n",
    "        return audio\n",
    "\n",
    "    resample_obj = T.Resample(sample_rate, new_sample_rate)\n",
    "    wave_form_resample = resample_obj(wave_form[:1, :])\n",
    "\n",
    "    num_channels = wave_form.shape[0]\n",
    "    if num_channels > 1:\n",
    "        # S'il y a un second canal.\n",
    "        wf_second_channel = resample_obj(wave_form[1:, :])\n",
    "        wave_form_resample = torch.cat([wave_form_resample, wf_second_channel])\n",
    "\n",
    "    return (wave_form_resample, new_sample_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
